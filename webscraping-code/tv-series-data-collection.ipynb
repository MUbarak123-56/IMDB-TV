{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below to go into previous directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ganiy\\OneDrive\\Documents\\IMDB-TV\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the codes below and run to create the directories if this is your first time running it. If it returns a statement stating that the directories already exists, proceed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%mkdir tv-series-data/\n",
    "#%mkdir tv-series-data-named/\n",
    "#%mkdir cumulative-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from requests import get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb Top 250 TV shows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section is to obtain the top 250 highest rated TV shows on IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TV Series List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url with the link to the top 250 highest rated TV shows list\n",
    "url = 'https://www.imdb.com/chart/toptv'\n",
    "response = get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting info about all the tv shows from the website\n",
    "containers = tv_soup.find_all('td', class_='titleColumn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers_rating = tv_soup.find_all('td', class_ = \"ratingColumn imdbRating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_list = []\n",
    "for i in range(len(containers_rating)):\n",
    "    rating = containers_rating[i].strong[\"title\"]\n",
    "    rating = rating[:3]\n",
    "    rating_list.append(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing out the encoded title of all the tv shows\n",
    "tv_list = []\n",
    "for i in range(0,len(containers)):\n",
    "    title = containers[i].a['href']\n",
    "    title = title.split(\"/\")[2]\n",
    "    tv_list.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list that extracts information about each tv such as \n",
    "# title, rating, total_votes, description, release year, its link and its encoded title\n",
    "comprehensive_list = []\n",
    "for tv in tv_list:\n",
    "    response = get('https://www.imdb.com/title/' + tv + \"/\")\n",
    "    tv_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    title_verbose = tv_soup.find('title').string\n",
    "    releaseYear = re.findall(r'[0-9][0-9][0-9][0-9]', title_verbose)\n",
    "    tv_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    #tv_title = tv_soup.find('title').string\n",
    "    #rating = tv_soup.find(\"span\", class_ = \"AggregateRatingButton__RatingScore-sc-1ll29m0-1 iTLWoV\").string\n",
    "    rating_count = tv_soup.find(\"div\", class_ =\"sc-7ab21ed2-3 dPVcnq\").string\n",
    "    tv_title = tv_soup.find(\"h1\", {\"data-testid\": \"hero-title-block__title\"}).string\n",
    "    description = tv_soup.find(\"span\", {\"data-testid\": \"plot-xl\"}).string\n",
    "    link = 'https://www.imdb.com/title/' + tv\n",
    "    encoded_title = tv\n",
    "    genres = tv_soup.find_all('a', class_ = \"GenresAndPlot__GenreChip-sc-cum89p-3 LKJMs ipc-chip ipc-chip--on-baseAlt\")\n",
    "    genre = \"\"\n",
    "    for g in genres:\n",
    "        genre += g.text + \", \"\n",
    "    genre = genre.strip(\", \")\n",
    "    comprehensive_list.append([tv_title, rating_count, description, releaseYear[0], link, encoded_title, genre])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### converting the comprehensive list into a data frame\n",
    "tv_best = pd.DataFrame(comprehensive_list, columns = [\"title\",\"total_votes\", \"show_desc\", \"year\", \"link\", \"encoded_title\", \"genre\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_best[\"rating\"] = rating_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_best[\"rating\"] = tv_best[\"rating\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_best = tv_best[[\"title\",\"rating\",\"total_votes\", \"show_desc\", \"year\", \"link\", \"genre\",\"encoded_title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting K to 000 for total votes\n",
    "tv_best[\"total_votes\"] = tv_best[\"total_votes\"].str.replace(\"K\", \"000\")\n",
    "tv_best[\"total_votes\"] = tv_best[\"total_votes\"].str.replace(\"M\", \"000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reseting index of data frame\n",
    "tv_best = tv_best.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a for loop to return a better result for the \n",
    "for i in range(len(tv_best)):\n",
    "    if \".\" in tv_best.loc[i, \"total_votes\"]:\n",
    "        tv_best.loc[i,\"total_votes\"] = tv_best.loc[i, \"total_votes\"][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ganiy\\Anaconda3\\envs\\Qiskit\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# replacing . with an empty string so total votes can be converted into integer\n",
    "tv_best[\"total_votes\"] = tv_best[\"total_votes\"].str.replace(\".\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming certain columns into integers\n",
    "tv_best[\"rating\"] = pd.to_numeric(tv_best[\"rating\"], downcast=\"float\")\n",
    "tv_best[\"total_votes\"] = tv_best[\"total_votes\"].astype(int)\n",
    "tv_best[\"year\"] = tv_best[\"year\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>show_desc</th>\n",
       "      <th>year</th>\n",
       "      <th>link</th>\n",
       "      <th>genre</th>\n",
       "      <th>encoded_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Planet Earth II</td>\n",
       "      <td>9.4</td>\n",
       "      <td>119000</td>\n",
       "      <td>David Attenborough returns with a new wildlife...</td>\n",
       "      <td>2016</td>\n",
       "      <td>https://www.imdb.com/title/tt5491994</td>\n",
       "      <td></td>\n",
       "      <td>tt5491994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Breaking Bad</td>\n",
       "      <td>9.4</td>\n",
       "      <td>1700000</td>\n",
       "      <td>A high school chemistry teacher diagnosed with...</td>\n",
       "      <td>2008</td>\n",
       "      <td>https://www.imdb.com/title/tt0903747</td>\n",
       "      <td></td>\n",
       "      <td>tt0903747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Planet Earth</td>\n",
       "      <td>9.4</td>\n",
       "      <td>186000</td>\n",
       "      <td>Emmy Award-winning, 11 episodes, five years in...</td>\n",
       "      <td>2006</td>\n",
       "      <td>https://www.imdb.com/title/tt0795176</td>\n",
       "      <td></td>\n",
       "      <td>tt0795176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Band of Brothers</td>\n",
       "      <td>9.4</td>\n",
       "      <td>428000</td>\n",
       "      <td>The story of Easy Company of the U.S. Army 101...</td>\n",
       "      <td>2001</td>\n",
       "      <td>https://www.imdb.com/title/tt0185906</td>\n",
       "      <td></td>\n",
       "      <td>tt0185906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chernobyl</td>\n",
       "      <td>9.3</td>\n",
       "      <td>675000</td>\n",
       "      <td>In April 1986, an explosion at the Chernobyl n...</td>\n",
       "      <td>2019</td>\n",
       "      <td>https://www.imdb.com/title/tt7366338</td>\n",
       "      <td></td>\n",
       "      <td>tt7366338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Love, Death &amp; Robots</td>\n",
       "      <td>8.4</td>\n",
       "      <td>134000</td>\n",
       "      <td>A collection of animated short stories that sp...</td>\n",
       "      <td>2019</td>\n",
       "      <td>https://www.imdb.com/title/tt9561862</td>\n",
       "      <td></td>\n",
       "      <td>tt9561862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Foyle's War</td>\n",
       "      <td>8.4</td>\n",
       "      <td>15000</td>\n",
       "      <td>As WWII rages, DCS Foyle fights his own war on...</td>\n",
       "      <td>2002</td>\n",
       "      <td>https://www.imdb.com/title/tt0310455</td>\n",
       "      <td></td>\n",
       "      <td>tt0310455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Jesus of Nazareth</td>\n",
       "      <td>8.4</td>\n",
       "      <td>22000</td>\n",
       "      <td>Beginning before the Nativity and extending th...</td>\n",
       "      <td>1977</td>\n",
       "      <td>https://www.imdb.com/title/tt0075520</td>\n",
       "      <td></td>\n",
       "      <td>tt0075520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Clannad: After Story</td>\n",
       "      <td>8.4</td>\n",
       "      <td>11000</td>\n",
       "      <td>None</td>\n",
       "      <td>2008</td>\n",
       "      <td>https://www.imdb.com/title/tt1298820</td>\n",
       "      <td></td>\n",
       "      <td>tt1298820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Luther</td>\n",
       "      <td>8.4</td>\n",
       "      <td>133000</td>\n",
       "      <td>DCI John Luther is a near-genius murder detect...</td>\n",
       "      <td>2010</td>\n",
       "      <td>https://www.imdb.com/title/tt1474684</td>\n",
       "      <td></td>\n",
       "      <td>tt1474684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    title  rating  total_votes  \\\n",
       "0         Planet Earth II     9.4       119000   \n",
       "1            Breaking Bad     9.4      1700000   \n",
       "2            Planet Earth     9.4       186000   \n",
       "3        Band of Brothers     9.4       428000   \n",
       "4               Chernobyl     9.3       675000   \n",
       "..                    ...     ...          ...   \n",
       "245  Love, Death & Robots     8.4       134000   \n",
       "246           Foyle's War     8.4        15000   \n",
       "247     Jesus of Nazareth     8.4        22000   \n",
       "248  Clannad: After Story     8.4        11000   \n",
       "249                Luther     8.4       133000   \n",
       "\n",
       "                                             show_desc  year  \\\n",
       "0    David Attenborough returns with a new wildlife...  2016   \n",
       "1    A high school chemistry teacher diagnosed with...  2008   \n",
       "2    Emmy Award-winning, 11 episodes, five years in...  2006   \n",
       "3    The story of Easy Company of the U.S. Army 101...  2001   \n",
       "4    In April 1986, an explosion at the Chernobyl n...  2019   \n",
       "..                                                 ...   ...   \n",
       "245  A collection of animated short stories that sp...  2019   \n",
       "246  As WWII rages, DCS Foyle fights his own war on...  2002   \n",
       "247  Beginning before the Nativity and extending th...  1977   \n",
       "248                                               None  2008   \n",
       "249  DCI John Luther is a near-genius murder detect...  2010   \n",
       "\n",
       "                                     link genre encoded_title  \n",
       "0    https://www.imdb.com/title/tt5491994           tt5491994  \n",
       "1    https://www.imdb.com/title/tt0903747           tt0903747  \n",
       "2    https://www.imdb.com/title/tt0795176           tt0795176  \n",
       "3    https://www.imdb.com/title/tt0185906           tt0185906  \n",
       "4    https://www.imdb.com/title/tt7366338           tt7366338  \n",
       "..                                    ...   ...           ...  \n",
       "245  https://www.imdb.com/title/tt9561862           tt9561862  \n",
       "246  https://www.imdb.com/title/tt0310455           tt0310455  \n",
       "247  https://www.imdb.com/title/tt0075520           tt0075520  \n",
       "248  https://www.imdb.com/title/tt1298820           tt1298820  \n",
       "249  https://www.imdb.com/title/tt1474684           tt1474684  \n",
       "\n",
       "[250 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tv_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_desc = tv_best[[\"encoded_title\", \"show_desc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_best.to_csv(\"cumulative-data/IMDb_top_250.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting episode data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions and code were used to collect data from the TV episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tv_special(link):\n",
    "    # This function is for tv that do not have numerous seasons but rather one season.\n",
    "    # Due to their layout it is harder to obtain information about the episodes by season.\n",
    "    # Hence, it was optimal to go to the page with all the episodes in ascending order.\n",
    "    \n",
    "    # obtaining url to obtain tv info\n",
    "    url = 'https://www.imdb.com/title/' + link + \"/\"\n",
    "    response = get(url)\n",
    "    # parsing content of request\n",
    "    tv_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # collecting title data\n",
    "    tv_title = tv_soup.find(\"h1\", {\"data-testid\": \"hero-title-block__title\"}).string\n",
    "    # collecting data of total number of episodes\n",
    "    total_episodes = int(tv_soup.find(\"span\", class_ = \"ipc-title__subtext\").text)\n",
    "    # obtaining new link for extracting tv information by episode\n",
    "    new_link = \"https://www.imdb.com/search/title/?series=\" + link + \"&view=simple&sort=release_date,asc\"\n",
    "    response = get(new_link)\n",
    "    # parsing content of request\n",
    "    tv_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    l = tv_soup.find_all(\"span\", class_ = \"lister-item-header\")\n",
    "    comprehend = []\n",
    "    episode = 0\n",
    "    season = 1\n",
    "    for u in l:\n",
    "        u = str(u)\n",
    "        # finding all tv titles\n",
    "        tv = re.findall(r'/title/tt[0-9]*/', u)\n",
    "        v = str(tv[1]).split(\"/\")[2]\n",
    "        url_n = 'https://www.imdb.com/title/' + v + \"/\"\n",
    "        response = get(url_n)\n",
    "        tv_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # obtaining tv title\n",
    "        title = tv_soup.find(\"h1\", {\"data-testid\": \"hero-title-block__title\"}).text\n",
    "        episode += 1\n",
    "        # checking to make sure rating is not empty\n",
    "        if tv_soup.find(\"span\", class_ = \"sc-7ab21ed2-1 jGRxWM\")== None:\n",
    "            rating = \"0\"\n",
    "        else:\n",
    "            # recording rating\n",
    "            rating = tv_soup.find(\"span\", class_ = \"sc-7ab21ed2-1 jGRxWM\").text\n",
    "        # checking to make sure total_votes is not empty\n",
    "        if tv_soup.find(\"div\", class_ =\"sc-7ab21ed2-3 dPVcnq\") == None:\n",
    "            total_votes = \"0\"\n",
    "        else:\n",
    "            # recording total_votes\n",
    "            total_votes = tv_soup.find(\"div\", class_ =\"sc-7ab21ed2-3 dPVcnq\").text\n",
    "        # checking to make sure air date is not empty\n",
    "        if tv_soup.find(\"li\", class_ =\"ipc-inline-list__item\") == None:\n",
    "            airdate = \"\"\n",
    "        else:\n",
    "            # recording airdate\n",
    "            airdate = tv_soup.find(\"li\", class_ =\"ipc-inline-list__item\").text\n",
    "        desc = tv_soup.find(\"span\", class_ = \"sc-16ede01-2 gXUyNh\").text\n",
    "        comprehend.append([tv_title, season, episode, title, airdate, rating, total_votes, desc, url_n, link])\n",
    "\n",
    "    # The while loop below is used to extend to following pages so their information can be extracted\n",
    "    n = 51\n",
    "    while n < total_episodes:\n",
    "        # obtaining urls\n",
    "        url = \"https://www.imdb.com/search/title/?series=\" + link + \"&view=simple&sort=release_date,asc&start=\" + str(n) + \"&ref_=adv_nxt\"\n",
    "        response = get(url)\n",
    "        tv_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        l = tv_soup.find_all(\"span\", class_ = \"lister-item-header\")\n",
    "        for u in l:\n",
    "            u = str(u)\n",
    "            tv = re.findall(r'/title/tt[0-9]*/', u)\n",
    "            v = str(tv[1]).split(\"/\")[2]\n",
    "            url_n = 'https://www.imdb.com/title/' + v + \"/\"\n",
    "            response = get(url_n)\n",
    "            tv_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            title = tv_soup.find(\"h1\", {\"data-testid\": \"hero-title-block__title\"}).text\n",
    "            episode += 1\n",
    "            if tv_soup.find(\"span\", class_ = \"sc-7ab21ed2-1 jGRxWM\")== None:\n",
    "                rating = \"0\"\n",
    "            else:\n",
    "                rating = tv_soup.find(\"span\", class_ = \"sc-7ab21ed2-1 jGRxWM\").text\n",
    "            if tv_soup.find(\"div\", class_ =\"sc-7ab21ed2-3 dPVcnq\") == None:\n",
    "                total_votes = \"0\"\n",
    "            else:\n",
    "                total_votes = tv_soup.find(\"div\", class_ =\"sc-7ab21ed2-3 dPVcnq\").text\n",
    "            if tv_soup.find(\"li\", class_ =\"ipc-inline-list__item\") == None:\n",
    "                airdate = \"\"\n",
    "            else:\n",
    "                airdate = tv_soup.find(\"li\", class_ =\"ipc-inline-list__item\").text\n",
    "            desc = tv_soup.find(\"span\", class_ = \"sc-16ede01-2 gXUyNh\").text\n",
    "            comprehend.append([tv_title, season, episode, title, airdate, rating, total_votes, desc, url_n, link])\n",
    "        n += 50\n",
    "    return (comprehend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_recorder(episode):\n",
    "    # This function is used for recording episodes information for tvs that have multiple seasons\n",
    "    \n",
    "    # recording episode number\n",
    "    episode_number = int(episode.meta['content'])\n",
    "    # recording episode title\n",
    "    title = episode.a['title']\n",
    "    # recording air date\n",
    "    airdate = episode.find('div', class_='airdate').text.strip()\n",
    "    # making sure the airdate value is not empty\n",
    "    if len(airdate.split(\" \")) >= 3:\n",
    "        # transforming the format of the airdate\n",
    "        new = airdate.split(\" \")\n",
    "        new = [new[1], new[0], new[2]]\n",
    "        new[1] = new[1] + \",\"\n",
    "        new = \" \".join(new)\n",
    "        airdate = new.replace(\".\", \"\")\n",
    "        if episode.find('span', class_='ipl-rating-star__rating')== None:\n",
    "            rating = \"0\"\n",
    "        else:\n",
    "            rating = episode.find('span', class_='ipl-rating-star__rating').text\n",
    "        if episode.find('span', class_='ipl-rating-star__total-votes') == None:\n",
    "            total_votes = \"0\"\n",
    "        else:\n",
    "            total_votes = episode.find('span', class_='ipl-rating-star__total-votes').text\n",
    "    else:\n",
    "        airdate = \"\"\n",
    "        rating = \"0\"\n",
    "        total_votes = \"0\"\n",
    "    desc = episode.find('div', class_='item_description').text.strip()\n",
    "    return [episode_number, title, airdate, rating, total_votes, desc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tv_output(link):\n",
    "    tv_episodes = []\n",
    "    url = 'https://www.imdb.com/title/' + link + \"/\"\n",
    "    response = get(url)\n",
    "    tv_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tv_title = tv_soup.find(\"h1\", {\"data-testid\": \"hero-title-block__title\"}).string\n",
    "    if tv_soup.find(\"select\", class_ = \"ipc-simple-select__input\")== None or tv_soup.find(\"select\", class_ = \"ipc-simple-select__input\")[\"aria-label\"][2:] != \"seasons\":\n",
    "        tv_episodes.extend(tv_special(link))\n",
    "    else: \n",
    "        n = int(tv_soup.find(\"select\", class_ = \"ipc-simple-select__input\")[\"aria-label\"][0])\n",
    "        for sn in range(1,n+1):\n",
    "            response = get('https://www.imdb.com/title/' + link + '/episodes?season=' + str(sn))\n",
    "\n",
    "            page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            episode_containers = page_html.find_all('div', class_ = 'info')\n",
    "\n",
    "            \n",
    "            for episodes in episode_containers:\n",
    "                season = sn\n",
    "                url_n1 = \"https://www.imdb.com\" + episodes.find('strong').a[\"href\"]\n",
    "                #episode_number = episodes.meta['content']\n",
    "                #title = episodes.a['title']\n",
    "                #airdate = episodes.find('div', class_='airdate').text.strip()\n",
    "                #episode_data = [tv_title, season, episode_number, title, airdate, rating, total_votes, desc]\n",
    "                episode_data = [tv_title, season]\n",
    "                l = episode_recorder(episodes)\n",
    "                episode_data.extend(l)\n",
    "                episode_data.append(url_n1)\n",
    "                episode_data.append(link)\n",
    "                tv_episodes.append(episode_data)\n",
    "    return(tv_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mini dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mini dataset extracts information about the first five TV shows to showcase how the final dataset will look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out function to make sure it can collect data about tv\n",
    "l = []\n",
    "for tv in tv_list[:5]:\n",
    "    l.extend(tv_output(tv))\n",
    "df = pd.DataFrame(l, columns = ['tv', 'season', 'episode_number', 'title', 'airdate', 'rating', 'total_votes', 'episode_desc', 'episode_url', 'encoded_title'])\n",
    "df = df.merge(encoded_desc, on = \"encoded_title\", how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ganiy\\Anaconda3\\envs\\Qiskit\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  \"\"\"\n",
      "C:\\Users\\ganiy\\Anaconda3\\envs\\Qiskit\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df[\"airdate\"] = df[\"airdate\"].str.replace(\"Episode aired \", \"\")\n",
    "df[\"airdate\"] = df[\"airdate\"].str.replace(\"Episode airs \", \"\")\n",
    "df[\"total_votes\"] = df[\"total_votes\"].str.replace(\"K\", \"000\")\n",
    "df[\"total_votes\"] = df[\"total_votes\"].str.replace(\",\", \"\")\n",
    "df[\"total_votes\"] = df[\"total_votes\"].str.replace(\"(\", \"\")\n",
    "df[\"total_votes\"] = df[\"total_votes\"].str.replace(\")\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    if \".\" in df.loc[i, \"total_votes\"]:\n",
    "        df.loc[i,\"total_votes\"] = df.loc[i, \"total_votes\"][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ganiy\\Anaconda3\\envs\\Qiskit\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df[\"total_votes\"] = df[\"total_votes\"].str.replace(\".\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming certain columns into integers\n",
    "df[\"season\"] = df[\"season\"].astype(int)\n",
    "df[\"episode_number\"] = df[\"episode_number\"].astype(int)\n",
    "df[\"rating\"] = pd.to_numeric(df[\"rating\"], downcast=\"float\")\n",
    "df[\"total_votes\"] = df[\"total_votes\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tv</th>\n",
       "      <th>season</th>\n",
       "      <th>episode_number</th>\n",
       "      <th>title</th>\n",
       "      <th>airdate</th>\n",
       "      <th>rating</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>episode_desc</th>\n",
       "      <th>episode_url</th>\n",
       "      <th>encoded_title</th>\n",
       "      <th>show_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Planet Earth II</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Islands</td>\n",
       "      <td>Feb 18, 2017</td>\n",
       "      <td>9.3</td>\n",
       "      <td>4600</td>\n",
       "      <td>Wildlife documentary series with David Attenbo...</td>\n",
       "      <td>https://www.imdb.com/title/tt6142646/</td>\n",
       "      <td>tt5491994</td>\n",
       "      <td>David Attenborough returns with a new wildlife...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planet Earth II</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Mountains</td>\n",
       "      <td>Feb 25, 2017</td>\n",
       "      <td>8.9</td>\n",
       "      <td>3500</td>\n",
       "      <td>The wildlife documentary series with David Att...</td>\n",
       "      <td>https://www.imdb.com/title/tt6209126/</td>\n",
       "      <td>tt5491994</td>\n",
       "      <td>David Attenborough returns with a new wildlife...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Planet Earth II</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Jungles</td>\n",
       "      <td>Mar 4, 2017</td>\n",
       "      <td>8.7</td>\n",
       "      <td>3100</td>\n",
       "      <td>Jungles provide the richest habitats on the pl...</td>\n",
       "      <td>https://www.imdb.com/title/tt6209130/</td>\n",
       "      <td>tt5491994</td>\n",
       "      <td>David Attenborough returns with a new wildlife...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Planet Earth II</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Deserts</td>\n",
       "      <td>Mar 11, 2017</td>\n",
       "      <td>8.6</td>\n",
       "      <td>2800</td>\n",
       "      <td>The world's deserts force animals to come up w...</td>\n",
       "      <td>https://www.imdb.com/title/tt6209134/</td>\n",
       "      <td>tt5491994</td>\n",
       "      <td>David Attenborough returns with a new wildlife...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Planet Earth II</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Grasslands</td>\n",
       "      <td>Mar 18, 2017</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2600</td>\n",
       "      <td>Grasslands cover one quarter of all land and s...</td>\n",
       "      <td>https://www.imdb.com/title/tt6209140/</td>\n",
       "      <td>tt5491994</td>\n",
       "      <td>David Attenborough returns with a new wildlife...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Chernobyl</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1:23:45</td>\n",
       "      <td>May 6, 2019</td>\n",
       "      <td>9.4</td>\n",
       "      <td>51000</td>\n",
       "      <td>Plant workers and firefighters put their lives...</td>\n",
       "      <td>https://www.imdb.com/title/tt8162428/</td>\n",
       "      <td>tt7366338</td>\n",
       "      <td>In April 1986, an explosion at the Chernobyl n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Chernobyl</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Please Remain Calm</td>\n",
       "      <td>May 13, 2019</td>\n",
       "      <td>9.6</td>\n",
       "      <td>49000</td>\n",
       "      <td>With untold millions at risk, Ulana makes a de...</td>\n",
       "      <td>https://www.imdb.com/title/tt8482972/</td>\n",
       "      <td>tt7366338</td>\n",
       "      <td>In April 1986, an explosion at the Chernobyl n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Chernobyl</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Open Wide, O Earth</td>\n",
       "      <td>May 20, 2019</td>\n",
       "      <td>9.5</td>\n",
       "      <td>46000</td>\n",
       "      <td>Valery creates a detailed plan to decontaminat...</td>\n",
       "      <td>https://www.imdb.com/title/tt9166672/</td>\n",
       "      <td>tt7366338</td>\n",
       "      <td>In April 1986, an explosion at the Chernobyl n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Chernobyl</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>The Happiness of All Mankind</td>\n",
       "      <td>May 27, 2019</td>\n",
       "      <td>9.4</td>\n",
       "      <td>43000</td>\n",
       "      <td>Valery and Boris attempt to find solutions to ...</td>\n",
       "      <td>https://www.imdb.com/title/tt9166678/</td>\n",
       "      <td>tt7366338</td>\n",
       "      <td>In April 1986, an explosion at the Chernobyl n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Chernobyl</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Vichnaya Pamyat</td>\n",
       "      <td>Jun 3, 2019</td>\n",
       "      <td>9.8</td>\n",
       "      <td>58000</td>\n",
       "      <td>Valery, Boris and Ulana risk their lives and r...</td>\n",
       "      <td>https://www.imdb.com/title/tt9166696/</td>\n",
       "      <td>tt7366338</td>\n",
       "      <td>In April 1986, an explosion at the Chernobyl n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tv  season  episode_number                         title  \\\n",
       "0   Planet Earth II       1               1                       Islands   \n",
       "1   Planet Earth II       1               2                     Mountains   \n",
       "2   Planet Earth II       1               3                       Jungles   \n",
       "3   Planet Earth II       1               4                       Deserts   \n",
       "4   Planet Earth II       1               5                    Grasslands   \n",
       "..              ...     ...             ...                           ...   \n",
       "89        Chernobyl       1               1                       1:23:45   \n",
       "90        Chernobyl       1               2            Please Remain Calm   \n",
       "91        Chernobyl       1               3            Open Wide, O Earth   \n",
       "92        Chernobyl       1               4  The Happiness of All Mankind   \n",
       "93        Chernobyl       1               5               Vichnaya Pamyat   \n",
       "\n",
       "         airdate  rating  total_votes  \\\n",
       "0   Feb 18, 2017     9.3         4600   \n",
       "1   Feb 25, 2017     8.9         3500   \n",
       "2    Mar 4, 2017     8.7         3100   \n",
       "3   Mar 11, 2017     8.6         2800   \n",
       "4   Mar 18, 2017     8.5         2600   \n",
       "..           ...     ...          ...   \n",
       "89   May 6, 2019     9.4        51000   \n",
       "90  May 13, 2019     9.6        49000   \n",
       "91  May 20, 2019     9.5        46000   \n",
       "92  May 27, 2019     9.4        43000   \n",
       "93   Jun 3, 2019     9.8        58000   \n",
       "\n",
       "                                         episode_desc  \\\n",
       "0   Wildlife documentary series with David Attenbo...   \n",
       "1   The wildlife documentary series with David Att...   \n",
       "2   Jungles provide the richest habitats on the pl...   \n",
       "3   The world's deserts force animals to come up w...   \n",
       "4   Grasslands cover one quarter of all land and s...   \n",
       "..                                                ...   \n",
       "89  Plant workers and firefighters put their lives...   \n",
       "90  With untold millions at risk, Ulana makes a de...   \n",
       "91  Valery creates a detailed plan to decontaminat...   \n",
       "92  Valery and Boris attempt to find solutions to ...   \n",
       "93  Valery, Boris and Ulana risk their lives and r...   \n",
       "\n",
       "                              episode_url encoded_title  \\\n",
       "0   https://www.imdb.com/title/tt6142646/     tt5491994   \n",
       "1   https://www.imdb.com/title/tt6209126/     tt5491994   \n",
       "2   https://www.imdb.com/title/tt6209130/     tt5491994   \n",
       "3   https://www.imdb.com/title/tt6209134/     tt5491994   \n",
       "4   https://www.imdb.com/title/tt6209140/     tt5491994   \n",
       "..                                    ...           ...   \n",
       "89  https://www.imdb.com/title/tt8162428/     tt7366338   \n",
       "90  https://www.imdb.com/title/tt8482972/     tt7366338   \n",
       "91  https://www.imdb.com/title/tt9166672/     tt7366338   \n",
       "92  https://www.imdb.com/title/tt9166678/     tt7366338   \n",
       "93  https://www.imdb.com/title/tt9166696/     tt7366338   \n",
       "\n",
       "                                            show_desc  \n",
       "0   David Attenborough returns with a new wildlife...  \n",
       "1   David Attenborough returns with a new wildlife...  \n",
       "2   David Attenborough returns with a new wildlife...  \n",
       "3   David Attenborough returns with a new wildlife...  \n",
       "4   David Attenborough returns with a new wildlife...  \n",
       "..                                                ...  \n",
       "89  In April 1986, an explosion at the Chernobyl n...  \n",
       "90  In April 1986, an explosion at the Chernobyl n...  \n",
       "91  In April 1986, an explosion at the Chernobyl n...  \n",
       "92  In April 1986, an explosion at the Chernobyl n...  \n",
       "93  In April 1986, an explosion at the Chernobyl n...  \n",
       "\n",
       "[94 rows x 11 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was used to store all the datasets in different data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = []\n",
    "count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ganiy\\Anaconda3\\envs\\Qiskit\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\ganiy\\Anaconda3\\envs\\Qiskit\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\ganiy\\Anaconda3\\envs\\Qiskit\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for tv in tv_list[:5]: # to obtain all the datasets, replace tv_list[:5] with tv_list\n",
    "    new_list = tv_output(tv)\n",
    "    df = pd.DataFrame(new_list, columns = ['tv', 'season', 'episode_number', 'title', 'airdate', 'rating', 'total_votes', 'episode_desc', 'episode_url', 'encoded_title'])\n",
    "    df = df.merge(encoded_desc, on = \"encoded_title\", how = \"inner\")\n",
    "    df[\"airdate\"] = df[\"airdate\"].str.replace(\"Episode aired \", \"\")\n",
    "    df[\"airdate\"] = df[\"airdate\"].str.replace(\"Episode airs \", \"\")\n",
    "    df[\"total_votes\"] = df[\"total_votes\"].str.replace(\"K\", \"000\")\n",
    "    df[\"total_votes\"] = df[\"total_votes\"].str.replace(\",\", \"\")\n",
    "    df[\"total_votes\"] = df[\"total_votes\"].str.replace(\"(\", \"\")\n",
    "    df[\"total_votes\"] = df[\"total_votes\"].str.replace(\")\", \"\")\n",
    "    for i in range(len(df)):\n",
    "        if \".\" in df.loc[i, \"total_votes\"]:\n",
    "            df.loc[i,\"total_votes\"] = df.loc[i, \"total_votes\"][:-1]\n",
    "    df[\"total_votes\"] = df[\"total_votes\"].str.replace(\".\", \"\")\n",
    "    df[\"season\"] = df[\"season\"].astype(int)\n",
    "    df[\"episode_number\"] = df[\"episode_number\"].astype(int)\n",
    "    df[\"rating\"] = pd.to_numeric(df[\"rating\"], downcast=\"float\")\n",
    "    df[\"total_votes\"] = df[\"total_votes\"].astype(int)\n",
    "    df = df[df[\"rating\"] > 0]\n",
    "    df.to_csv(\"tv-series-data/\" + tv + \".csv\", index = False)\n",
    "    # splitting the tv column into multiple columns so as to obtain tv name\n",
    "    new = df[\"tv\"].str.split(\" \", expand = True)\n",
    "    new = new.fillna(\"\")\n",
    "    word = \"-\".join(new.iloc[0,])\n",
    "    name = word.replace(\":\", \"\").strip(\"-\").lower()\n",
    "    name = name.replace(\";\", \"\")\n",
    "    name = name.replace(\"!\",\"\")\n",
    "    name = name.replace(\".\", \"\")\n",
    "    name = name.replace(\"/\", \"\")\n",
    "    name = name.replace(\"'\", \"\")\n",
    "    name = name.replace(\",\", \"\")\n",
    "    name = name.replace(\"?\", \"\")\n",
    "    if name in name_list:\n",
    "        name += str(count)\n",
    "    name_list.append(name)\n",
    "    df.to_csv(\"tv-series-data-named/\" + name + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below combines all the tv shows' datasets into one huge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['tv', 'season', 'episode_number', 'title', 'airdate', 'rating', 'total_votes', 'episode_desc', 'episode_url', 'encoded_title', 'show_desc'])\n",
    "for tv in tv_list: \n",
    "    if os.path.isfile(\"tv-series-data/\" + tv + \".csv\"):\n",
    "        df1 = pd.read_csv(\"tv-series-data/\" + tv + \".csv\")\n",
    "        df = pd.concat([df1, df], axis = 0)\n",
    "        df = df.reset_index(drop = True)\n",
    "df.to_csv(\"cumulative-data/tv_dataset.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
